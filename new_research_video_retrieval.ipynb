{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qem6csVVoTDM",
        "outputId": "4cf126b4-7cf6-4fd3-dd6a-ce6fe7994d1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1yS2_Kz0JIT",
        "outputId": "4b8d7108-00c1-4c89-e62d-94a78f05adf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ALBEF'...\n",
            "remote: Enumerating objects: 353, done.\u001b[K\n",
            "remote: Counting objects: 100% (208/208), done.\u001b[K\n",
            "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
            "remote: Total 353 (delta 112), reused 104 (delta 104), pack-reused 145 (from 1)\u001b[K\n",
            "Receiving objects: 100% (353/353), 71.56 MiB | 16.78 MiB/s, done.\n",
            "Resolving deltas: 100% (134/134), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/salesforce/ALBEF.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_Jb4DEX0PxA",
        "outputId": "32a266ad-73f4-48de-d2cc-c5ce37871461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ALBEF\n",
            "--2025-06-11 06:24:56--  https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/ALBEF.pth\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.118.207, 172.217.194.207, 142.251.10.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.118.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3493862125 (3.3G) [application/octet-stream]\n",
            "Saving to: ‘ALBEF.pth’\n",
            "\n",
            "ALBEF.pth           100%[===================>]   3.25G  23.2MB/s    in 2m 36s  \n",
            "\n",
            "2025-06-11 06:27:33 (21.3 MB/s) - ‘ALBEF.pth’ saved [3493862125/3493862125]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ALBEF\n",
        "!wget https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/ALBEF.pth\n",
        "!wget https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/mscoco.pth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSEbCAAZnbrO",
        "outputId": "1463421d-6b64-4c4a-dc2a-279af28694cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ruamel.yaml\n",
            "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting timm==0.4.9\n",
            "  Downloading timm-0.4.9-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Collecting decord\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.11/dist-packages (from timm==0.4.9) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm==0.4.9) (0.21.0+cu124)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.9) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.9) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.4->timm==0.4.9)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.4->timm==0.4.9)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.4->timm==0.4.9)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.4->timm==0.4.9)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.4->timm==0.4.9)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.4->timm==0.4.9)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.4->timm==0.4.9)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.4->timm==0.4.9)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.4->timm==0.4.9)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.9) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.9) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.9) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.4->timm==0.4.9)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.9) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.4.9) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4->timm==0.4.9) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm==0.4.9) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4->timm==0.4.9) (3.0.2)\n",
            "Downloading timm-0.4.9-py3-none-any.whl (346 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.1/346.1 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml.clib, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, decord, ruamel.yaml, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.15\n",
            "    Uninstalling timm-1.0.15:\n",
            "      Successfully uninstalled timm-1.0.15\n",
            "Successfully installed decord-0.6.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 timm-0.4.9\n"
          ]
        }
      ],
      "source": [
        "!pip3 install ruamel.yaml timm==0.4.9 einops transformers gdown decord opencv-python scikit-learn\n",
        "# apt update && apt install -y libgl1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5egACFpoLjzf"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ7gxE5ULp96"
      },
      "outputs": [],
      "source": [
        "%cd mkdir /content/data\n",
        "!wget https://www.robots.ox.ac.uk/~maxbain/frozen-in-time/data/MSRVTT.zip -P data; unzip data/MSRVTT.zip -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX9bWKkWLjkr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "msrvtt_dir = '/content/drive/MyDrive/Intern_FPT/AI/DATASET/MSRVTT/videos/all'\n",
        "\n",
        "if os.path.exists(msrvtt_dir):\n",
        "  mp4_count = 0\n",
        "  for root, dirs, files in os.walk(msrvtt_dir):\n",
        "    for file in files:\n",
        "      if file.endswith(\".mp4\"):\n",
        "        mp4_count += 1\n",
        "  print(f\"Number of .mp4 files in {msrvtt_dir}: {mp4_count}\")\n",
        "else:\n",
        "  print(f\"Directory not found: {msrvtt_dir}\")\n",
        "  print(\"Please run the preceding cell to download and extract the data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IyuXFQ3LmE1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGjARZ_TKTLs"
      },
      "source": [
        "## create train, test, val json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YihlZ-t6FeK5",
        "outputId": "996b9aca-ad32-4bd4-d9a0-9666145183f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"info\": {\n",
            "    \"contributor\": \"...\",\n",
            "    \"version\": \"...\",\n",
            "    \"year\": \"...\",\n",
            "    \"data_created\": \"...\",\n",
            "    \"description\": \"...\"\n",
            "  },\n",
            "  \"images\": [\n",
            "    {\n",
            "      \"id\": \"...\"\n",
            "    }\n",
            "  ],\n",
            "  \"licenses\": [\n",
            "    {\n",
            "      \"url\": \"...\",\n",
            "      \"name\": \"...\"\n",
            "    }\n",
            "  ],\n",
            "  \"type\": \"...\",\n",
            "  \"annotations\": [\n",
            "    {\n",
            "      \"caption\": \"...\",\n",
            "      \"image_id\": \"...\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def simplify_json_structure(data, level=0, seen=None):\n",
        "    if seen is None:\n",
        "        seen = {}\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        result = {}\n",
        "        level_keys = seen.setdefault(level, set())\n",
        "\n",
        "        for key, value in data.items():\n",
        "            if key not in level_keys:\n",
        "                level_keys.add(key)\n",
        "                result[key] = simplify_json_structure(value, level + 1, seen)\n",
        "        return result\n",
        "\n",
        "    elif isinstance(data, list):\n",
        "        if len(data) > 0:\n",
        "            return [simplify_json_structure(data[0], level, seen)]\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    else:\n",
        "        return \"...\"\n",
        "\n",
        "# Load JSON file\n",
        "with open(\"/content/drive/MyDrive/Intern_FPT/AI/DATASET/MSRVTT/annotation/MSR_VTT.json\", \"r\") as f:\n",
        "    json_data = json.load(f)\n",
        "\n",
        "# Simplify and pretty print\n",
        "simplified = simplify_json_structure(json_data)\n",
        "print(json.dumps(simplified, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwSx4FYxHM0N",
        "outputId": "fdb342cb-32b4-4605-cd1f-065920c2aa4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/ALBEF/prepare_msrvtt_data.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/ALBEF/prepare_msrvtt_data.py\n",
        "import json\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split # Cần cài đặt: pip install scikit-learn\n",
        "import random\n",
        "\n",
        "# --- Cấu hình Đường dẫn ---\n",
        "# Thay đổi đường dẫn này để khớp với vị trí thư mục MSRVTT của bạn\n",
        "MSRVTT_ROOT = '/content/drive/MyDrive/Intern_FPT/AI/DATASET/MSRVTT/'\n",
        "\n",
        "FULL_ANNOTATION_FILE = os.path.join(MSRVTT_ROOT, 'annotation', 'MSR_VTT.json')\n",
        "TRAIN_VIDEO_IDS_FILE = os.path.join(MSRVTT_ROOT, 'high-quality/structured-symlinks', 'test_list_mini.txt')\n",
        "TEST_VIDEO_IDS_FILE = os.path.join(MSRVTT_ROOT, 'high-quality/structured-symlinks', 'train_list_full_first20.txt')\n",
        "\n",
        "OUTPUT_ANNOTATION_DIR = os.path.join(MSRVTT_ROOT, 'annotation')\n",
        "OUTPUT_TRAIN_FILE = os.path.join(OUTPUT_ANNOTATION_DIR, 'msrvtt_train.json')\n",
        "OUTPUT_VAL_FILE = os.path.join(OUTPUT_ANNOTATION_DIR, 'msrvtt_val.json')\n",
        "OUTPUT_TEST_FILE = os.path.join(OUTPUT_ANNOTATION_DIR, 'msrvtt_test.json')\n",
        "\n",
        "VAL_SPLIT_RATIO = 0.1 # Tỷ lệ phần trăm video từ tập huấn luyện để tạo tập validation\n",
        "\n",
        "# Đặt một seed ngẫu nhiên để đảm bảo các kết quả phân chia là nhất quán\n",
        "random.seed(42)\n",
        "\n",
        "# --- Bước 1: Tải tất cả dữ liệu cần thiết ---\n",
        "print(f\"Đang tải file annotation đầy đủ từ: {FULL_ANNOTATION_FILE}\")\n",
        "with open(FULL_ANNOTATION_FILE, 'r') as f:\n",
        "    full_ann = json.load(f)\n",
        "\n",
        "print(f\"Đang tải ID video huấn luyện từ: {TRAIN_VIDEO_IDS_FILE}\")\n",
        "with open(TRAIN_VIDEO_IDS_FILE, 'r') as f:\n",
        "    train_video_ids_raw = [line.strip() for line in f]\n",
        "\n",
        "print(f\"Đang tải ID video kiểm tra từ: {TEST_VIDEO_IDS_FILE}\")\n",
        "with open(TEST_VIDEO_IDS_FILE, 'r') as f:\n",
        "    test_video_ids_raw = [line.strip() for line in f]\n",
        "\n",
        "# Chuyển đổi sang set để tra cứu nhanh hơn\n",
        "train_video_ids_set = set(train_video_ids_raw)\n",
        "test_video_ids_set = set(test_video_ids_raw)\n",
        "\n",
        "# --- Bước 2: Tạo ánh xạ từ video_id đến các chú thích của nó ---\n",
        "print(\"Đang tạo ánh xạ từ ID video đến các chú thích...\")\n",
        "video_annotations_map = {}\n",
        "# Duyệt qua phần \"annotations\" của JSON\n",
        "for ann_entry in full_ann['annotations']:\n",
        "    video_id = ann_entry['image_id'] # Trong MSR_VTT.json, 'image_id' chính là ID video\n",
        "    caption = ann_entry['caption']\n",
        "\n",
        "    # Định dạng lại entry để khớp với cấu trúc mong muốn của dataset\n",
        "    formatted_entry = {\"video_id\": video_id, \"caption\": caption}\n",
        "\n",
        "    if video_id not in video_annotations_map:\n",
        "        video_annotations_map[video_id] = []\n",
        "    video_annotations_map[video_id].append(formatted_entry)\n",
        "\n",
        "# --- Bước 3: Đổ dữ liệu vào danh sách ban đầu cho huấn luyện và kiểm tra ---\n",
        "print(\"Đang đổ dữ liệu vào danh sách huấn luyện và kiểm tra ban đầu...\")\n",
        "\n",
        "# Thu thập tất cả các ID video duy nhất thuộc tập huấn luyện ban đầu và có chú thích\n",
        "train_videos_for_split = []\n",
        "for video_id in train_video_ids_set:\n",
        "    if video_id in video_annotations_map:\n",
        "        train_videos_for_split.append(video_id)\n",
        "    # else:\n",
        "    #     print(f\"Cảnh báo: ID video huấn luyện {video_id} không tìm thấy trong MSR_VTT.json annotations.\")\n",
        "\n",
        "# Thu thập tất cả các chú thích cho tập kiểm tra\n",
        "test_ann = []\n",
        "for video_id in test_video_ids_set:\n",
        "    if video_id in video_annotations_map:\n",
        "        test_ann.extend(video_annotations_map[video_id])\n",
        "    # else:\n",
        "    #     print(f\"Cảnh báo: ID video kiểm tra {video_id} không tìm thấy trong MSR_VTT.json annotations.\")\n",
        "\n",
        "# Xáo trộn danh sách ID video huấn luyện để đảm bảo việc phân chia train/val ngẫu nhiên và nhất quán\n",
        "random.shuffle(train_videos_for_split)\n",
        "\n",
        "# --- Bước 4: Tạo phân chia train/validation từ các video huấn luyện ---\n",
        "print(f\"Đang phân chia các video huấn luyện (tổng {len(train_videos_for_split)}) thành train/val ({VAL_SPLIT_RATIO} cho val)...\")\n",
        "if VAL_SPLIT_RATIO > 0:\n",
        "    final_train_video_ids, val_video_ids = train_test_split(\n",
        "        train_videos_for_split, test_size=VAL_SPLIT_RATIO, random_state=42\n",
        "    )\n",
        "else:\n",
        "    final_train_video_ids = train_videos_for_split\n",
        "    val_video_ids = []\n",
        "\n",
        "final_train_ann = []\n",
        "val_ann = []\n",
        "\n",
        "for video_id in final_train_video_ids:\n",
        "    final_train_ann.extend(video_annotations_map[video_id])\n",
        "\n",
        "for video_id in val_video_ids:\n",
        "    val_ann.extend(video_annotations_map[video_id])\n",
        "\n",
        "print(f\"Số lượng chú thích tập huấn luyện: {len(final_train_ann)}\")\n",
        "print(f\"Số lượng chú thích tập validation: {len(val_ann)}\")\n",
        "print(f\"Số lượng chú thích tập kiểm tra: {len(test_ann)}\")\n",
        "\n",
        "# --- Bước 5: Lưu các file annotation mới ---\n",
        "print(f\"Đang lưu các file annotation vào: {OUTPUT_ANNOTATION_DIR}\")\n",
        "os.makedirs(OUTPUT_ANNOTATION_DIR, exist_ok=True)\n",
        "\n",
        "with open(OUTPUT_TRAIN_FILE, 'w') as f:\n",
        "    json.dump(final_train_ann, f, indent=4)\n",
        "print(f\"Đã lưu annotation huấn luyện vào {OUTPUT_TRAIN_FILE}\")\n",
        "\n",
        "with open(OUTPUT_VAL_FILE, 'w') as f:\n",
        "    json.dump(val_ann, f, indent=4)\n",
        "print(f\"Đã lưu annotation validation vào {OUTPUT_VAL_FILE}\")\n",
        "\n",
        "with open(OUTPUT_TEST_FILE, 'w') as f:\n",
        "    json.dump(test_ann, f, indent=4)\n",
        "print(f\"Đã lưu annotation kiểm tra vào {OUTPUT_TEST_FILE}\")\n",
        "\n",
        "print(\"Hoàn tất quá trình phân chia dữ liệu!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27cGfLqnHW9o",
        "outputId": "32e7465e-9467-4801-fd91-59f0f3289c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đang tải file annotation đầy đủ từ: /content/drive/MyDrive/Intern_FPT/AI/DATASET/MSRVTT/annotation/MSR_VTT.json\n",
            "Đang tải ID video huấn luyện từ: /content/drive/MyDrive/Intern_FPT/AI/DATASET/MSRVTT/high-quality/structured-symlinks/test_list_mini.txt\n",
            "Đang tải ID video kiểm tra từ: /content/drive/MyDrive/Intern_FPT/AI/DATASET/MSRVTT/high-quality/structured-symlinks/train_list_full_first20.txt\n",
            "Đang tạo ánh xạ từ ID video đến các chú thích...\n",
            "Đang đổ dữ liệu vào danh sách huấn luyện và kiểm tra ban đầu...\n",
            "Đang phân chia các video huấn luyện (tổng 5) thành train/val (0.1 cho val)...\n",
            "Số lượng chú thích tập huấn luyện: 80\n",
            "Số lượng chú thích tập validation: 20\n",
            "Số lượng chú thích tập kiểm tra: 400\n",
            "Đang lưu các file annotation vào: /content/drive/MyDrive/Intern_FPT/AI/DATASET/MSRVTT/annotation\n",
            "Đã lưu annotation huấn luyện vào /content/drive/MyDrive/Intern_FPT/AI/DATASET/MSRVTT/annotation/msrvtt_train.json\n",
            "Đã lưu annotation validation vào /content/drive/MyDrive/Intern_FPT/AI/DATASET/MSRVTT/annotation/msrvtt_val.json\n",
            "Đã lưu annotation kiểm tra vào /content/drive/MyDrive/Intern_FPT/AI/DATASET/MSRVTT/annotation/msrvtt_test.json\n",
            "Hoàn tất quá trình phân chia dữ liệu!\n"
          ]
        }
      ],
      "source": [
        "!python /content/ALBEF/prepare_msrvtt_data.py\n",
        "# python3 prepare_msrvtt_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_xvXFMvKvqN"
      },
      "source": [
        "## main compponent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvgP9sgq_M4a",
        "outputId": "fddab086-e360-4b04-9961-2c28a458c50b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/ALBEF/dataset/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/ALBEF/dataset/__init__.py\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Xóa các import cũ không cần thiết từ caption_dataset nếu không sử dụng cho các tác vụ khác\n",
        "# Nếu `pretrain_dataset` vẫn được dùng, giữ lại nó hoặc di chuyển định nghĩa của nó vào đây.\n",
        "# Giả sử bạn vẫn cần `pretrain_dataset` nên giữ import này.\n",
        "from dataset.caption_dataset import pretrain_dataset\n",
        "from dataset.nlvr_dataset import nlvr_dataset\n",
        "from dataset.ve_dataset import ve_dataset\n",
        "from dataset.vqa_dataset import vqa_dataset\n",
        "from dataset.grounding_dataset import grounding_dataset\n",
        "\n",
        "from dataset.randaugment import RandomAugment\n",
        "\n",
        "# Cần thêm các imports này ở đầu file\n",
        "import os\n",
        "import json # Để đọc file annotation JSON cho MSR-VTT\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "# Import decord\n",
        "try:\n",
        "    from decord import VideoReader, cpu\n",
        "    # Bạn có thể uncomment dòng dưới nếu muốn decord trả về tensor torch trực tiếp\n",
        "    # decord.bridge.set_bridge(\"torch\")\n",
        "except ImportError:\n",
        "    print(\"Warning: Decord not found. Please install it (pip install decord) for video loading. Placeholder will be used if not installed.\")\n",
        "    VideoReader = None # Placeholder if decord is not available\n",
        "\n",
        "\n",
        "# --- Đảm bảo các transforms đã được định nghĩa ---\n",
        "# Giữ nguyên như ALBEF gốc, nhưng lưu ý các transform này áp dụng cho TỪNG khung hình\n",
        "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "# image_res được lấy từ config, nhưng ở đây chúng ta dùng 224 hardcode để đảm bảo đúng.\n",
        "# Trong hàm create_dataset dưới đây, chúng ta sẽ truyền config['image_res'] vào.\n",
        "# Các transform này sẽ được tạo trong create_dataset để có thể sử dụng config['image_res']\n",
        "# Đây là cách chính xác hơn:\n",
        "# pretrain_transform = transforms.Compose([...])\n",
        "# train_transform = transforms.Compose([...])\n",
        "# test_transform = transforms.Compose([...])\n",
        "\n",
        "\n",
        "# --- Định nghĩa các lớp Dataset mới cho Video Retrieval ---\n",
        "\n",
        "class VideoRetrievalTrainDataset(Dataset):\n",
        "    def __init__(self, ann_file, transform, video_root, num_frames_per_video):\n",
        "        self.ann = json.load(open(ann_file, 'r'))\n",
        "        self.transform = transform\n",
        "        self.video_root = video_root\n",
        "        self.num_frames_per_video = num_frames_per_video\n",
        "\n",
        "        # Trích xuất văn bản và video_ids từ annotation\n",
        "        self.text = [entry['caption'] for entry in self.ann]\n",
        "        self.video_ids = [entry['video_id'] for entry in self.ann]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ann)\n",
        "\n",
        "    def _load_video_frames(self, video_path):\n",
        "        \"\"\"Tải và lấy mẫu khung hình từ một video.\"\"\"\n",
        "        if VideoReader is None:\n",
        "            print(f\"Error: Decord not installed. Cannot load video {video_path}. Returning black frames.\")\n",
        "            return torch.zeros(self.num_frames_per_video, 3, 224, 224) # Hardcoded 224,224 for safety if image_res not available\n",
        "\n",
        "        try:\n",
        "            vr = VideoReader(video_path, ctx=cpu(0))\n",
        "            total_frames = len(vr)\n",
        "\n",
        "            if total_frames == 0:\n",
        "                print(f\"Warning: Video {video_path} has 0 frames. Returning black frames.\")\n",
        "                return torch.zeros(self.num_frames_per_video, 3, 224, 224) # Placeholder\n",
        "\n",
        "            indices = np.linspace(0, total_frames - 1, self.num_frames_per_video, dtype=int)\n",
        "\n",
        "            frames = []\n",
        "            for idx in indices:\n",
        "                frame = Image.fromarray(vr[idx].asnumpy())\n",
        "                frames.append(self.transform(frame))\n",
        "\n",
        "            return torch.stack(frames)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading video {video_path}: {e}. Returning black frames.\")\n",
        "            return torch.zeros(self.num_frames_per_video, 3, 224, 224)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ann_entry = self.ann[index]\n",
        "        caption = ann_entry['caption']\n",
        "        video_id = ann_entry['video_id']\n",
        "\n",
        "        video_path = os.path.join(self.video_root, f\"{video_id}.mp4\")\n",
        "\n",
        "        video_tensor = self._load_video_frames(video_path)\n",
        "\n",
        "        return video_tensor, caption, index\n",
        "\n",
        "\n",
        "class VideoRetrievalEvalDataset(Dataset):\n",
        "    def __init__(self, ann_file, transform, video_root, num_frames_per_video):\n",
        "        self.ann = json.load(open(ann_file, 'r'))\n",
        "        self.transform = transform\n",
        "        self.video_root = video_root\n",
        "        self.num_frames_per_video = num_frames_per_video\n",
        "\n",
        "        self.text = [ann['caption'] for ann in self.ann]\n",
        "        self.video_ids = [ann['video_id'] for ann in self.ann]\n",
        "\n",
        "        self.txt2img = list(range(len(self.text)))\n",
        "        self.img2txt = [[i] for i in range(len(self.video_ids))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ann)\n",
        "\n",
        "    def _load_video_frames(self, video_path):\n",
        "        if VideoReader is None:\n",
        "            print(f\"Error: Decord not installed. Cannot load video {video_path}. Returning black frames.\")\n",
        "            return torch.zeros(self.num_frames_per_video, 3, 224, 224)\n",
        "\n",
        "        try:\n",
        "            vr = VideoReader(video_path, ctx=cpu(0))\n",
        "            total_frames = len(vr)\n",
        "            if total_frames == 0:\n",
        "                print(f\"Warning: Video {video_path} has 0 frames. Returning black frames.\")\n",
        "                return torch.zeros(self.num_frames_per_video, 3, 224, 224)\n",
        "            indices = np.linspace(0, total_frames - 1, self.num_frames_per_video, dtype=int)\n",
        "            frames = []\n",
        "            for idx in indices:\n",
        "                frame = Image.fromarray(vr[idx].asnumpy())\n",
        "                frames.append(self.transform(frame))\n",
        "            return torch.stack(frames)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading video {video_path}: {e}. Returning black frames.\")\n",
        "            return torch.zeros(self.num_frames_per_video, 3, 224, 224)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ann_entry = self.ann[index]\n",
        "        video_id = ann_entry['video_id']\n",
        "        video_path = os.path.join(self.video_root, f\"{video_id}.mp4\")\n",
        "\n",
        "        video_tensor = self._load_video_frames(video_path)\n",
        "\n",
        "        return video_tensor, index\n",
        "\n",
        "\n",
        "# --- Sửa đổi hàm create_dataset để sử dụng các lớp dataset mới ---\n",
        "def create_dataset(dataset, config):\n",
        "\n",
        "    normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "    # Transforms được tạo ở đây để có thể sử dụng config['image_res']\n",
        "    pretrain_transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(config['image_res'],scale=(0.2, 1.0), interpolation=Image.BICUBIC),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            RandomAugment(2,7,isPIL=True,augs=['Identity','AutoContrast','Equalize','Brightness','Sharpness',\n",
        "                                              'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Rotate']),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "    train_transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(config['image_res'],scale=(0.5, 1.0), interpolation=Image.BICUBIC),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            RandomAugment(2,7,isPIL=True,augs=['Identity','AutoContrast','Equalize','Brightness','Sharpness',\n",
        "                                              'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Rotate']),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((config['image_res'],config['image_res']),interpolation=Image.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "        ])\n",
        "\n",
        "    if dataset=='pretrain':\n",
        "        dataset = pretrain_dataset(config['train_file'], pretrain_transform)\n",
        "        return dataset\n",
        "\n",
        "    elif dataset=='re':\n",
        "        # Sử dụng các lớp dataset video mới\n",
        "        train_dataset = VideoRetrievalTrainDataset(config['train_file'][0], train_transform, config['image_root'], config['num_frames_per_video'])\n",
        "        val_dataset = VideoRetrievalEvalDataset(config['val_file'], test_transform, config['image_root'], config['num_frames_per_video'])\n",
        "        test_dataset = VideoRetrievalEvalDataset(config['test_file'], test_transform, config['image_root'], config['num_frames_per_video'])\n",
        "        return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "    elif dataset=='vqa':\n",
        "        train_dataset = vqa_dataset(config['train_file'], train_transform, config['vqa_root'], config['vg_root'], split='train')\n",
        "        vqa_test_dataset = vqa_dataset(config['test_file'], test_transform, config['vqa_root'], config['vg_root'], split='test', answer_list=config['answer_list'])\n",
        "        return train_dataset, vqa_test_dataset\n",
        "\n",
        "    elif dataset=='nlvr':\n",
        "        train_dataset = nlvr_dataset(config['train_file'], train_transform, config['image_root'])\n",
        "        val_dataset = nlvr_dataset(config['val_file'], test_transform, config['image_root'])\n",
        "        test_dataset = nlvr_dataset(config['test_file'], test_transform, config['image_root'])\n",
        "        return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "    elif dataset=='ve':\n",
        "        train_dataset = ve_dataset(config['train_file'], train_transform, config['image_root'])\n",
        "        val_dataset = ve_dataset(config['val_file'], test_transform, config['image_root'])\n",
        "        test_dataset = ve_dataset(config['test_file'], test_transform, config['image_root'])\n",
        "        return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "    elif dataset=='grounding':\n",
        "        train_transform = transforms.Compose([\n",
        "                transforms.Resize((config['image_res'],config['image_res']),interpolation=Image.BICUBIC),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                RandomAugment(2,7,isPIL=True,augs=['Identity','AutoContrast','Equalize','Brightness','Sharpness',\n",
        "                                                  'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Rotate']),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])\n",
        "        train_dataset = grounding_dataset(config['train_file'], train_transform, config['image_root'], mode='train')\n",
        "        test_dataset = grounding_dataset(config['test_file'], test_transform, config['image_root'], mode='test')\n",
        "        return train_dataset, test_dataset\n",
        "\n",
        "def vqa_collate_fn(batch):\n",
        "    image_list, question_list, answer_list, weight_list, n = [], [], [], [], []\n",
        "    for image, question, answer, weights in batch:\n",
        "        image_list.append(image)\n",
        "        question_list.append(question)\n",
        "        weight_list += weights\n",
        "        answer_list += answer\n",
        "        n.append(len(answer))\n",
        "    return torch.stack(image_list,dim=0), question_list, answer_list, torch.Tensor(weight_list), n\n",
        "\n",
        "\n",
        "def create_sampler(datasets, shuffles, num_tasks, global_rank):\n",
        "    samplers = []\n",
        "    for dataset,shuffle in zip(datasets,shuffles):\n",
        "        sampler = torch.utils.data.DistributedSampler(dataset, num_replicas=num_tasks, rank=global_rank, shuffle=shuffle)\n",
        "        samplers.append(sampler)\n",
        "    return samplers\n",
        "\n",
        "\n",
        "def create_loader(datasets, samplers, batch_size, num_workers, is_trains, collate_fns):\n",
        "    loaders = []\n",
        "    for dataset,sampler,bs,n_worker,is_train,collate_fn in zip(datasets,samplers,batch_size,num_workers,is_trains,collate_fns):\n",
        "        if is_train:\n",
        "            shuffle = (sampler is None)\n",
        "            drop_last = True\n",
        "        else:\n",
        "            shuffle = False\n",
        "            drop_last = False\n",
        "        loader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=bs,\n",
        "            num_workers=n_worker,\n",
        "            pin_memory=True,\n",
        "            sampler=sampler,\n",
        "            shuffle=shuffle,\n",
        "            collate_fn=collate_fn,\n",
        "            drop_last=drop_last,\n",
        "        )\n",
        "        loaders.append(loader)\n",
        "    return loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1_aA0-n0lOD",
        "outputId": "e1279d30-47ec-47d9-b5ae-0bcba3733cda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/ALBEF/configs/Retrieval_msrvtt.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/ALBEF/configs/Retrieval_msrvtt.yaml\n",
        "# Đường dẫn đến các file annotation và video của MSR-VTT\n",
        "# LƯU Ý: Bạn cần thay đổi '/path/to/your/DATASET/MSRVTT/' thành đường dẫn thực tế trên hệ thống của bạn.\n",
        "# Ví dụ: nếu bạn đặt thư mục DATASET trong thư mục dự án ALBEF, thì đường dẫn có thể là './DATASET/MSRVTT/'\n",
        "# Hoặc nếu bạn đặt nó trong /content/, thì là '/content/DATASET/MSRVTT/'\n",
        "msrvtt_root: '/content/drive/MyDrive/Intern_FPT/AI/DATASET/MSRVTT/' # Đảm bảo đường dẫn này chính xác\n",
        "\n",
        "# File annotation chính chứa tất cả captions và video_ids\n",
        "# Các file này sẽ được tạo bởi script prepare_msrvtt_data.py\n",
        "train_file:  ['/content/drive/MyDrive/Intern_FPT/AI/DATASET/MSRVTT/annotation/msrvtt_train.json']\n",
        "val_file: '/content/drive/MyDrive/Intern_FPT/AI/DATASET/MSRVTT/annotation/msrvtt_val.json'\n",
        "test_file: '/content/drive/MyDrive/Intern_FPT/AI/DATASET/MSRVTT/annotation/msrvtt_test.json'\n",
        "\n",
        "# Đường dẫn đến thư mục chứa các video thực tế\n",
        "# Dựa trên cấu trúc của bạn, các video nằm trong `MSRVTT/videos/all/`\n",
        "image_root: '/content/drive/MyDrive/Intern_FPT/AI/DATASET/MSRVTT/videos/all/'\n",
        "\n",
        "# Video-specific parameters cho VoP Giai đoạn 0\n",
        "num_frames_per_video: 12 # Số lượng khung hình sẽ lấy mẫu từ mỗi video, như trong bài báo VoP\n",
        "\n",
        "# Các tham số của mô hình ALBEF\n",
        "bert_config: 'configs/config_bert.json'\n",
        "\n",
        "# MSR-VTT thường sử dụng image_res 224x224\n",
        "image_res: 224 # Thay đổi từ 384 của flickr sang 224\n",
        "\n",
        "batch_size_train: 32\n",
        "batch_size_test: 64\n",
        "\n",
        "queue_size: 65536\n",
        "momentum: 0.995\n",
        "vision_width: 768\n",
        "embed_dim: 256\n",
        "temp: 0.07\n",
        "k_test: 128\n",
        "\n",
        "alpha: 0.4\n",
        "distill: True\n",
        "warm_up: True\n",
        "\n",
        "optimizer: {opt: adamW, lr: 1e-5, weight_decay: 0.02}\n",
        "schedular: {sched: cosine, lr: 1e-5, epochs: 10, min_lr: 1e-6, decay_rate: 1, warmup_lr: 1e-5, cooldown_epochs: 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzdyFuCsnfyh",
        "outputId": "42e6b7a5-433a-4226-d748-21c35c5d4293"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/ALBEF/models/xbert.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/ALBEF/models/xbert.py\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch BERT model. \"\"\"\n",
        "\n",
        "import math\n",
        "import os\n",
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import Tensor, device, dtype, nn\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers.file_utils import (\n",
        "    ModelOutput,\n",
        "    add_code_sample_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    replace_return_docstrings,\n",
        ")\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "    CausalLMOutputWithCrossAttentions,\n",
        "    MaskedLMOutput,\n",
        "    MultipleChoiceModelOutput,\n",
        "    NextSentencePredictorOutput,\n",
        "    QuestionAnsweringModelOutput,\n",
        "    SequenceClassifierOutput,\n",
        "    TokenClassifierOutput,\n",
        ")\n",
        "from transformers.modeling_utils import (\n",
        "    PreTrainedModel,\n",
        "    apply_chunking_to_forward,\n",
        "    find_pruneable_heads_and_indices,\n",
        "    prune_linear_layer,\n",
        ")\n",
        "from transformers.utils import logging\n",
        "from transformers.models.bert.configuration_bert import BertConfig\n",
        "\n",
        "import transformers\n",
        "transformers.logging.set_verbosity_error()\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CONFIG_FOR_DOC = \"BertConfig\"\n",
        "_TOKENIZER_FOR_DOC = \"BertTokenizer\"\n",
        "\n",
        "BERT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"bert-base-uncased\",\n",
        "    \"bert-large-uncased\",\n",
        "    \"bert-base-cased\",\n",
        "    \"bert-large-cased\",\n",
        "    \"bert-base-multilingual-uncased\",\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    \"bert-base-chinese\",\n",
        "    \"bert-base-german-cased\",\n",
        "    \"bert-large-uncased-whole-word-masking\",\n",
        "    \"bert-large-cased-whole-word-masking\",\n",
        "    \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
        "    \"bert-large-cased-whole-word-masking-finetuned-squad\",\n",
        "    \"bert-base-cased-finetuned-mrpc\",\n",
        "    \"bert-base-german-dbmdz-cased\",\n",
        "    \"bert-base-german-dbmdz-uncased\",\n",
        "    \"cl-tohoku/bert-base-japanese\",\n",
        "    \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
        "    \"cl-tohoku/bert-base-japanese-char\",\n",
        "    \"cl-tohoku/bert-base-japanese-char-whole-word-masking\",\n",
        "    \"TurkuNLP/bert-base-finnish-cased-v1\",\n",
        "    \"TurkuNLP/bert-base-finnish-uncased-v1\",\n",
        "    \"wietsedv/bert-base-dutch-cased\",\n",
        "    # See all BERT models at https://huggingface.co/models?filter=bert\n",
        "]\n",
        "\n",
        "\n",
        "def load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n",
        "    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n",
        "    try:\n",
        "        import re\n",
        "\n",
        "        import numpy as np\n",
        "        import tensorflow as tf\n",
        "    except ImportError:\n",
        "        logger.error(\n",
        "            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n",
        "            \"https://www.tensorflow.org/install/ for installation instructions.\"\n",
        "        )\n",
        "        raise\n",
        "    tf_path = os.path.abspath(tf_checkpoint_path)\n",
        "    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n",
        "    # Load weights from TF model\n",
        "    init_vars = tf.train.list_variables(tf_path)\n",
        "    names = []\n",
        "    arrays = []\n",
        "    for name, shape in init_vars:\n",
        "        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n",
        "        array = tf.train.load_variable(tf_path, name)\n",
        "        names.append(name)\n",
        "        arrays.append(array)\n",
        "\n",
        "    for name, array in zip(names, arrays):\n",
        "        name = name.split(\"/\")\n",
        "        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
        "        # which are not required for using pretrained model\n",
        "        if any(\n",
        "            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n",
        "            for n in name\n",
        "        ):\n",
        "            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
        "            continue\n",
        "        pointer = model\n",
        "        for m_name in name:\n",
        "            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n",
        "                scope_names = re.split(r\"_(\\d+)\", m_name)\n",
        "            else:\n",
        "                scope_names = [m_name]\n",
        "            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n",
        "                pointer = getattr(pointer, \"weight\")\n",
        "            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n",
        "                pointer = getattr(pointer, \"bias\")\n",
        "            elif scope_names[0] == \"output_weights\":\n",
        "                pointer = getattr(pointer, \"weight\")\n",
        "            elif scope_names[0] == \"squad\":\n",
        "                pointer = getattr(pointer, \"classifier\")\n",
        "            else:\n",
        "                try:\n",
        "                    pointer = getattr(pointer, scope_names[0])\n",
        "                except AttributeError:\n",
        "                    logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
        "                    continue\n",
        "            if len(scope_names) >= 2:\n",
        "                num = int(scope_names[1])\n",
        "                pointer = pointer[num]\n",
        "        if m_name[-11:] == \"_embeddings\":\n",
        "            pointer = getattr(pointer, \"weight\")\n",
        "        elif m_name == \"kernel\":\n",
        "            array = np.transpose(array)\n",
        "        try:\n",
        "            assert (\n",
        "                pointer.shape == array.shape\n",
        "            ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n",
        "        except AssertionError as e:\n",
        "            e.args += (pointer.shape, array.shape)\n",
        "            raise\n",
        "        logger.info(\"Initialize PyTorch weight {}\".format(name))\n",
        "        pointer.data = torch.from_numpy(array)\n",
        "    return model\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
        "    ):\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "        if self.position_embedding_type == \"absolute\":\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            embeddings += position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config, is_cross_attention):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        if is_cross_attention:\n",
        "            self.key = nn.Linear(config.encoder_width, self.all_head_size)\n",
        "            self.value = nn.Linear(config.encoder_width, self.all_head_size)\n",
        "        else:\n",
        "            self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "            self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "        self.save_attention = False\n",
        "\n",
        "    def save_attn_gradients(self, attn_gradients):\n",
        "        self.attn_gradients = attn_gradients\n",
        "\n",
        "    def get_attn_gradients(self):\n",
        "        return self.attn_gradients\n",
        "\n",
        "    def save_attention_map(self, attention_map):\n",
        "        self.attention_map = attention_map\n",
        "\n",
        "    def get_attention_map(self):\n",
        "        return self.attention_map\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask # bs,1,1,257\n",
        "        elif past_key_value is not None:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            seq_length = hidden_states.size()[1]\n",
        "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        if is_cross_attention and self.save_attention:\n",
        "            self.save_attention_map(attention_probs)\n",
        "            attention_probs.register_hook(self.save_attn_gradients)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs_dropped = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs_dropped = attention_probs_dropped * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs_dropped, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        outputs = outputs + (past_key_value,)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config, is_cross_attention=False):\n",
        "        super().__init__()\n",
        "        self.self = BertSelfAttention(config, is_cross_attention)\n",
        "        self.output = BertSelfOutput(config)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        heads, index = find_pruneable_heads_and_indices(\n",
        "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
        "        )\n",
        "\n",
        "        # Prune linear layers\n",
        "        self.self.query = prune_linear_layer(self.self.query, index)\n",
        "        self.self.key = prune_linear_layer(self.self.key, index)\n",
        "        self.self.value = prune_linear_layer(self.self.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "        # Update hyper params and store pruned heads\n",
        "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
        "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        self_outputs = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config, layer_num):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = BertAttention(config)\n",
        "\n",
        "        self.has_cross_attention = (layer_num >= config.fusion_layer)\n",
        "        if self.has_cross_attention:\n",
        "            self.layer_num = layer_num\n",
        "            self.crossattention = BertAttention(config, is_cross_attention=True)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "\n",
        "        outputs = self_attention_outputs[1:-1]\n",
        "        present_key_value = self_attention_outputs[-1]\n",
        "\n",
        "        if self.has_cross_attention:\n",
        "            assert encoder_hidden_states is not None, \"encoder_hidden_states must be given for cross-attention layers\"\n",
        "\n",
        "            if type(encoder_hidden_states) == list:\n",
        "                cross_attention_outputs = self.crossattention(\n",
        "                    attention_output,\n",
        "                    attention_mask,\n",
        "                    head_mask,\n",
        "                    encoder_hidden_states[(self.layer_num-self.config.fusion_layer)%len(encoder_hidden_states)],\n",
        "                    encoder_attention_mask[(self.layer_num-self.config.fusion_layer)%len(encoder_hidden_states)],\n",
        "                    output_attentions=output_attentions,\n",
        "                )\n",
        "                attention_output = cross_attention_outputs[0]\n",
        "                outputs = outputs + cross_attention_outputs[1:-1]\n",
        "\n",
        "            else:\n",
        "                cross_attention_outputs = self.crossattention(\n",
        "                    attention_output,\n",
        "                    attention_mask,\n",
        "                    head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    output_attentions=output_attentions,\n",
        "                )\n",
        "                attention_output = cross_attention_outputs[0]\n",
        "                outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
        "        layer_output = apply_chunking_to_forward(\n",
        "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
        "        )\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        outputs = outputs + (present_key_value,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([BertLayer(config,i) for i in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True,\n",
        "        mode='multi_modal',\n",
        "    ):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "\n",
        "\n",
        "        if mode=='text':\n",
        "            start_layer = 0\n",
        "            output_layer = self.config.fusion_layer\n",
        "\n",
        "        elif mode=='fusion':\n",
        "            start_layer = self.config.fusion_layer\n",
        "            output_layer = self.config.num_hidden_layers\n",
        "\n",
        "        elif mode=='multi_modal':\n",
        "            start_layer = 0\n",
        "            output_layer = self.config.num_hidden_layers\n",
        "\n",
        "        for i in range(start_layer, output_layer):\n",
        "            layer_module = self.layer[i]\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "\n",
        "                if use_cache:\n",
        "                    logger.warn(\n",
        "                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n",
        "                        \"`use_cache=False`...\"\n",
        "                    )\n",
        "                    use_cache = False\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, past_key_value, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_cross_attentions,\n",
        "                ]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_decoder_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class BertPredictionHeadTransform(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.transform_act_fn = config.hidden_act\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLMPredictionHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.transform = BertPredictionHeadTransform(config)\n",
        "\n",
        "        # The output weights are the same as the input embeddings, but there is\n",
        "        # an output-only bias for each token.\n",
        "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "\n",
        "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
        "        self.decoder.bias = self.bias\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.decoder(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOnlyMLMHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.predictions = BertLMPredictionHead(config)\n",
        "\n",
        "    def forward(self, sequence_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        return prediction_scores\n",
        "\n",
        "\n",
        "class BertOnlyNSPHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, pooled_output):\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return seq_relationship_score\n",
        "\n",
        "\n",
        "class BertPreTrainingHeads(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.predictions = BertLMPredictionHead(config)\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, sequence_output, pooled_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return prediction_scores, seq_relationship_score\n",
        "\n",
        "\n",
        "class BertPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
        "    models.\n",
        "    \"\"\"\n",
        "\n",
        "    config_class = BertConfig\n",
        "    load_tf_weights = load_tf_weights_in_bert\n",
        "    base_model_prefix = \"bert\"\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\" Initialize the weights \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BertForPreTrainingOutput(ModelOutput):\n",
        "    \"\"\"\n",
        "    Output type of :class:`~transformers.BertForPreTraining`.\n",
        "    Args:\n",
        "        loss (`optional`, returned when ``labels`` is provided, ``torch.FloatTensor`` of shape :obj:`(1,)`):\n",
        "            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n",
        "            (classification) loss.\n",
        "        prediction_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
        "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
        "        seq_relationship_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, 2)`):\n",
        "            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n",
        "            before SoftMax).\n",
        "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
        "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
        "            sequence_length, sequence_length)`.\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "            heads.\n",
        "    \"\"\"\n",
        "\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    prediction_logits: torch.FloatTensor = None\n",
        "    seq_relationship_logits: torch.FloatTensor = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "\n",
        "\n",
        "BERT_START_DOCSTRING = r\"\"\"\n",
        "    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n",
        "    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n",
        "    pruning heads etc.)\n",
        "    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n",
        "    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to\n",
        "    general usage and behavior.\n",
        "    Parameters:\n",
        "        config (:class:`~transformers.BertConfig`): Model configuration class with all the parameters of the model.\n",
        "            Initializing with a config file does not load the weights associated with the model, only the\n",
        "            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n",
        "            weights.\n",
        "\"\"\"\n",
        "\n",
        "BERT_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`):\n",
        "            Indices of input sequence tokens in the vocabulary.\n",
        "            Indices can be obtained using :class:`~transformers.BertTokenizer`. See\n",
        "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
        "            details.\n",
        "            `What are input IDs? <../glossary.html#input-ids>`__\n",
        "        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "            `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "        token_type_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,\n",
        "            1]``:\n",
        "            - 0 corresponds to a `sentence A` token,\n",
        "            - 1 corresponds to a `sentence B` token.\n",
        "            `What are token type IDs? <../glossary.html#token-type-ids>`_\n",
        "        position_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,\n",
        "            config.max_position_embeddings - 1]``.\n",
        "            `What are position IDs? <../glossary.html#position-ids>`_\n",
        "        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n",
        "            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`({0}, hidden_size)`, `optional`):\n",
        "            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n",
        "            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n",
        "            vectors than the model's internal embedding lookup matrix.\n",
        "        output_attentions (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (:obj:`bool`, `optional`):\n",
        "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare Bert Model transformer outputting raw hidden-states without any specific head on top.\",\n",
        "    BERT_START_DOCSTRING,\n",
        ")\n",
        "class BertModel(BertPreTrainedModel):\n",
        "    \"\"\"\n",
        "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
        "    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
        "    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
        "    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
        "    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
        "    input to the forward pass.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, add_pooling_layer=True):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "\n",
        "        self.encoder = BertEncoder(config)\n",
        "\n",
        "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\"\n",
        "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
        "        class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        # tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=\"bert-base-uncased\",\n",
        "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "\n",
        "\n",
        "    def get_extended_attention_mask(self, attention_mask: Tensor, input_shape: Tuple[int], device: device, is_decoder: bool) -> Tensor:\n",
        "        \"\"\"\n",
        "        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
        "\n",
        "        Arguments:\n",
        "            attention_mask (:obj:`torch.Tensor`):\n",
        "                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
        "            input_shape (:obj:`Tuple[int]`):\n",
        "                The shape of the input to the model.\n",
        "            device: (:obj:`torch.device`):\n",
        "                The device of the input to the model.\n",
        "\n",
        "        Returns:\n",
        "            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n",
        "        \"\"\"\n",
        "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "        if attention_mask.dim() == 3:\n",
        "            extended_attention_mask = attention_mask[:, None, :, :]\n",
        "        elif attention_mask.dim() == 2:\n",
        "            # Provided a padding mask of dimensions [batch_size, seq_length]\n",
        "            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
        "            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "            if is_decoder:\n",
        "                batch_size, seq_length = input_shape\n",
        "                seq_ids = torch.arange(seq_length, device=device)\n",
        "                causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n",
        "                # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n",
        "                # causal and attention masks must have same type with pytorch version < 1.3\n",
        "                causal_mask = causal_mask.to(attention_mask.dtype)\n",
        "\n",
        "                if causal_mask.shape[1] < attention_mask.shape[1]:\n",
        "                    prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n",
        "                    causal_mask = torch.cat(\n",
        "                        [\n",
        "                            torch.ones(\n",
        "                                (batch_size, seq_length, prefix_seq_len), device=device, dtype=causal_mask.dtype\n",
        "                            ),\n",
        "                            causal_mask,\n",
        "                        ],\n",
        "                        axis=-1,\n",
        "                    )\n",
        "\n",
        "                extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
        "            else:\n",
        "                extended_attention_mask = attention_mask[:, None, None, :]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n",
        "                    input_shape, attention_mask.shape\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "        return extended_attention_mask\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "        is_decoder=False,\n",
        "        mode='multi_modal',\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
        "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
        "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
        "        use_cache (:obj:`bool`, `optional`):\n",
        "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
        "            decoding (see :obj:`past_key_values`).\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if is_decoder:\n",
        "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        else:\n",
        "            use_cache = False\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            batch_size, seq_length = input_shape\n",
        "            device = input_ids.device\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "            batch_size, seq_length = input_shape\n",
        "            device = inputs_embeds.device\n",
        "        elif encoder_embeds is not None:\n",
        "            input_shape = encoder_embeds.size()[:-1]\n",
        "            batch_size, seq_length = input_shape\n",
        "            device = encoder_embeds.device\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds or encoder_embeds\")\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape,\n",
        "                                                                                 device, is_decoder)\n",
        "\n",
        "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
        "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "        if encoder_hidden_states is not None:\n",
        "            if type(encoder_hidden_states) == list:\n",
        "                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states[0].size()\n",
        "            else:\n",
        "                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "\n",
        "            if type(encoder_attention_mask) == list:\n",
        "                encoder_extended_attention_mask = [self.invert_attention_mask(mask) for mask in encoder_attention_mask]\n",
        "            elif encoder_attention_mask is None:\n",
        "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "            else:\n",
        "                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "        else:\n",
        "            encoder_extended_attention_mask = None\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        if encoder_embeds is None:\n",
        "            embedding_output = self.embeddings(\n",
        "                input_ids=input_ids,\n",
        "                position_ids=position_ids,\n",
        "                token_type_ids=token_type_ids,\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                past_key_values_length=past_key_values_length,\n",
        "            )\n",
        "        else:\n",
        "            embedding_output = encoder_embeds\n",
        "\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_extended_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "            mode=mode,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
        "            last_hidden_state=sequence_output,\n",
        "            pooler_output=pooled_output,\n",
        "            past_key_values=encoder_outputs.past_key_values,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "            cross_attentions=encoder_outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Bert Model with two heads on top as done during the pretraining: a `masked language modeling` head and a `next\n",
        "    sentence prediction (classification)` head.\n",
        "    \"\"\",\n",
        "    BERT_START_DOCSTRING,\n",
        ")\n",
        "class BertForPreTraining(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertPreTrainingHeads(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.cls.predictions.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.cls.predictions.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @replace_return_docstrings(output_type=BertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        next_sentence_label=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape ``(batch_size, sequence_length)``, `optional`):\n",
        "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
        "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
        "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "        next_sentence_label (``torch.LongTensor`` of shape ``(batch_size,)``, `optional`):\n",
        "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n",
        "            (see :obj:`input_ids` docstring) Indices should be in ``[0, 1]``:\n",
        "            - 0 indicates sequence B is a continuation of sequence A,\n",
        "            - 1 indicates sequence B is a random sequence.\n",
        "        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n",
        "            Used to hide legacy arguments that have been deprecated.\n",
        "        Returns:\n",
        "        Example::\n",
        "            >>> from transformers import BertTokenizer, BertForPreTraining\n",
        "            >>> import torch\n",
        "            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "            >>> model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
        "            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "            >>> outputs = model(**inputs)\n",
        "            >>> prediction_logits = outputs.prediction_logits\n",
        "            >>> seq_relationship_logits = outputs.seq_relationship_logits\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
        "\n",
        "        total_loss = None\n",
        "        if labels is not None and next_sentence_label is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
        "            total_loss = masked_lm_loss + next_sentence_loss\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n",
        "            return ((total_loss,) + output) if total_loss is not None else output\n",
        "\n",
        "        return BertForPreTrainingOutput(\n",
        "            loss=total_loss,\n",
        "            prediction_logits=prediction_scores,\n",
        "            seq_relationship_logits=seq_relationship_score,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"Bert Model with a `language modeling` head on top for CLM fine-tuning. \"\"\", BERT_START_DOCSTRING\n",
        ")\n",
        "class BertLMHeadModel(BertPreTrainedModel):\n",
        "\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        self.cls = BertOnlyMLMHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.cls.predictions.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.cls.predictions.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        labels=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "        is_decoder=True,\n",
        "        reduction='mean',\n",
        "        mode='multi_modal',\n",
        "        soft_labels=None,\n",
        "        alpha=0,\n",
        "        return_logits=False,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
        "            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n",
        "            ignored (masked), the loss is only computed for the tokens with labels n ``[0, ..., config.vocab_size]``\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
        "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
        "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
        "        use_cache (:obj:`bool`, `optional`):\n",
        "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
        "            decoding (see :obj:`past_key_values`).\n",
        "        Returns:\n",
        "        Example::\n",
        "            >>> from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n",
        "            >>> import torch\n",
        "            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "            >>> config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
        "            >>> model = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)\n",
        "            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "            >>> outputs = model(**inputs)\n",
        "            >>> prediction_logits = outputs.logits\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        if labels is not None:\n",
        "            use_cache = False\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "            is_decoder=is_decoder,\n",
        "            mode=mode,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.cls(sequence_output)\n",
        "\n",
        "        if return_logits:\n",
        "            return prediction_scores[:, :-1, :].contiguous()\n",
        "\n",
        "        lm_loss = None\n",
        "        if labels is not None:\n",
        "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
        "            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
        "            labels = labels[:, 1:].contiguous()\n",
        "            loss_fct = CrossEntropyLoss(reduction=reduction)\n",
        "            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "            lm_loss = lm_loss.view(prediction_scores.size(0),-1).sum(1)\n",
        "\n",
        "        if soft_labels is not None:\n",
        "            loss_distill = -torch.sum(F.log_softmax(shifted_prediction_scores, dim=1)*soft_labels,dim=-1)\n",
        "            loss_distill = (loss_distill * (labels!=-100)).sum(1)\n",
        "            lm_loss = (1-alpha)*lm_loss + alpha*loss_distill\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithCrossAttentions(\n",
        "            loss=lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n",
        "        input_shape = input_ids.shape\n",
        "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.new_ones(input_shape)\n",
        "\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past is not None:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"past_key_values\": past,\n",
        "            \"encoder_hidden_states\": model_kwargs.get(\"encoder_hidden_states\", None),\n",
        "            \"encoder_attention_mask\": model_kwargs.get(\"encoder_attention_mask\", None),\n",
        "            \"is_decoder\": True,\n",
        "        }\n",
        "\n",
        "    def _reorder_cache(self, past, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past:\n",
        "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
        "        return reordered_past\n",
        "\n",
        "\n",
        "@add_start_docstrings(\"\"\"Bert Model with a `language modeling` head on top. \"\"\", BERT_START_DOCSTRING)\n",
        "class BertForMaskedLM(BertPreTrainedModel):\n",
        "\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        self.cls = BertOnlyMLMHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.cls.predictions.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.cls.predictions.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        # tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=\"bert-base-uncased\",\n",
        "        output_type=MaskedLMOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "        is_decoder=False,\n",
        "        mode='multi_modal',\n",
        "        soft_labels=None,\n",
        "        alpha=0,\n",
        "        return_logits=False,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
        "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
        "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "        \"\"\"\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_embeds=encoder_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "            is_decoder=is_decoder,\n",
        "            mode=mode,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.cls(sequence_output)\n",
        "\n",
        "        if return_logits:\n",
        "            return prediction_scores\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if soft_labels is not None:\n",
        "            loss_distill = -torch.sum(F.log_softmax(prediction_scores, dim=1)*soft_labels,dim=-1)\n",
        "            loss_distill = loss_distill[labels!=-100].mean()\n",
        "            masked_lm_loss = (1-alpha)*masked_lm_loss + alpha*loss_distill\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return MaskedLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_kwargs):\n",
        "        input_shape = input_ids.shape\n",
        "        effective_batch_size = input_shape[0]\n",
        "\n",
        "        #  add a dummy token\n",
        "        assert self.config.pad_token_id is not None, \"The PAD token should be defined for generation\"\n",
        "        attention_mask = torch.cat([attention_mask, attention_mask.new_zeros((attention_mask.shape[0], 1))], dim=-1)\n",
        "        dummy_token = torch.full(\n",
        "            (effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device\n",
        "        )\n",
        "        input_ids = torch.cat([input_ids, dummy_token], dim=1)\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"Bert Model with a `next sentence prediction (classification)` head on top. \"\"\",\n",
        "    BERT_START_DOCSTRING,\n",
        ")\n",
        "class BertForNextSentencePrediction(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertOnlyNSPHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @replace_return_docstrings(output_type=NextSentencePredictorOutput, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n",
        "            (see ``input_ids`` docstring). Indices should be in ``[0, 1]``:\n",
        "            - 0 indicates sequence B is a continuation of sequence A,\n",
        "            - 1 indicates sequence B is a random sequence.\n",
        "        Returns:\n",
        "        Example::\n",
        "            >>> from transformers import BertTokenizer, BertForNextSentencePrediction\n",
        "            >>> import torch\n",
        "            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "            >>> model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "            >>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
        "            >>> next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
        "            >>> encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n",
        "            >>> outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
        "            >>> logits = outputs.logits\n",
        "            >>> assert logits[0, 0] < logits[0, 1] # next sentence was random\n",
        "        \"\"\"\n",
        "\n",
        "        if \"next_sentence_label\" in kwargs:\n",
        "            warnings.warn(\n",
        "                \"The `next_sentence_label` argument is deprecated and will be removed in a future version, use `labels` instead.\",\n",
        "                FutureWarning,\n",
        "            )\n",
        "            labels = kwargs.pop(\"next_sentence_label\")\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        seq_relationship_scores = self.cls(pooled_output)\n",
        "\n",
        "        next_sentence_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            next_sentence_loss = loss_fct(seq_relationship_scores.view(-1, 2), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (seq_relationship_scores,) + outputs[2:]\n",
        "            return ((next_sentence_loss,) + output) if next_sentence_loss is not None else output\n",
        "\n",
        "        return NextSentencePredictorOutput(\n",
        "            loss=next_sentence_loss,\n",
        "            logits=seq_relationship_scores,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n",
        "    output) e.g. for GLUE tasks.\n",
        "    \"\"\",\n",
        "    BERT_START_DOCSTRING,\n",
        ")\n",
        "class BertForSequenceClassification(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        # tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=\"bert-base-uncased\",\n",
        "        output_type=SequenceClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n",
        "    softmax) e.g. for RocStories/SWAG tasks.\n",
        "    \"\"\",\n",
        "    BERT_START_DOCSTRING,\n",
        ")\n",
        "class BertForMultipleChoice(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        # tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=\"bert-base-uncased\",\n",
        "        output_type=MultipleChoiceModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n",
        "            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n",
        "            :obj:`input_ids` above)\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n",
        "\n",
        "        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n",
        "        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
        "        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
        "        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n",
        "        inputs_embeds = (\n",
        "            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n",
        "            if inputs_embeds is not None\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        reshaped_logits = logits.view(-1, num_choices)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(reshaped_logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (reshaped_logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return MultipleChoiceModelOutput(\n",
        "            loss=loss,\n",
        "            logits=reshaped_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Bert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n",
        "    Named-Entity-Recognition (NER) tasks.\n",
        "    \"\"\",\n",
        "    BERT_START_DOCSTRING,\n",
        ")\n",
        "class BertForTokenClassification(BertPreTrainedModel):\n",
        "\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        # tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=\"bert-base-uncased\",\n",
        "        output_type=TokenClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
        "            1]``.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)\n",
        "                active_labels = torch.where(\n",
        "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "                )\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n",
        "    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n",
        "    \"\"\",\n",
        "    BERT_START_DOCSTRING,\n",
        ")\n",
        "class BertForQuestionAnswering(BertPreTrainedModel):\n",
        "\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        # tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=\"bert-base-uncased\",\n",
        "        output_type=QuestionAnsweringModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        start_positions=None,\n",
        "        end_positions=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        total_loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions.clamp_(0, ignored_index)\n",
        "            end_positions.clamp_(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (start_logits, end_logits) + outputs[2:]\n",
        "            return ((total_loss,) + output) if total_loss is not None else output\n",
        "\n",
        "        return QuestionAnsweringModelOutput(\n",
        "            loss=total_loss,\n",
        "            start_logits=start_logits,\n",
        "            end_logits=end_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojJCWf8hInNc",
        "outputId": "1f6d8e9b-ad8b-45bb-dbb7-11aed81284ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/ALBEF/models/model_retrieval.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/ALBEF/models/model_retrieval.py\n",
        "from functools import partial\n",
        "from models.vit import VisionTransformer\n",
        "from models.xbert import BertConfig, BertModel\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ALBEF(nn.Module):\n",
        "    def __init__(self,\n",
        "                 text_encoder = None,\n",
        "                 tokenizer = None,\n",
        "                 config = None,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.distill = config['distill']\n",
        "        embed_dim = config['embed_dim']\n",
        "        vision_width = config['vision_width']\n",
        "\n",
        "        # Thêm num_frames_per_video vào config để model biết số khung hình\n",
        "        self.num_frames_per_video = config['num_frames_per_video']\n",
        "\n",
        "        self.visual_encoder = VisionTransformer(\n",
        "            img_size=config['image_res'], patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
        "            mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "        bert_config = BertConfig.from_json_file(config['bert_config'])\n",
        "        self.text_encoder = BertModel.from_pretrained(text_encoder, config=bert_config, add_pooling_layer=False)\n",
        "\n",
        "        text_width = self.text_encoder.config.hidden_size\n",
        "        self.vision_proj = nn.Linear(vision_width, embed_dim)\n",
        "        self.text_proj = nn.Linear(text_width, embed_dim)\n",
        "\n",
        "        self.temp = nn.Parameter(torch.ones([]) * config['temp'])\n",
        "        self.queue_size = config['queue_size']\n",
        "        self.momentum = config['momentum']\n",
        "        self.itm_head = nn.Linear(text_width, 2)\n",
        "\n",
        "        # create momentum models\n",
        "        self.visual_encoder_m = VisionTransformer(\n",
        "            img_size=config['image_res'], patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
        "            mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "        self.vision_proj_m = nn.Linear(vision_width, embed_dim)\n",
        "        self.text_encoder_m = BertModel.from_pretrained(text_encoder, config=bert_config, add_pooling_layer=False)\n",
        "        self.text_proj_m = nn.Linear(text_width, embed_dim)\n",
        "\n",
        "        self.model_pairs = [[self.visual_encoder,self.visual_encoder_m],\n",
        "                            [self.vision_proj,self.vision_proj_m],\n",
        "                            [self.text_encoder,self.text_encoder_m],\n",
        "                            [self.text_proj,self.text_proj_m],\n",
        "                           ]\n",
        "        self.copy_params()\n",
        "\n",
        "        # create the queue\n",
        "        self.register_buffer(\"image_queue\", torch.randn(embed_dim, self.queue_size))\n",
        "        self.register_buffer(\"text_queue\", torch.randn(embed_dim, self.queue_size))\n",
        "        self.register_buffer(\"idx_queue\", torch.full((1,self.queue_size),-100))\n",
        "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
        "\n",
        "        self.image_queue = nn.functional.normalize(self.image_queue, dim=0)\n",
        "        self.text_queue = nn.functional.normalize(self.text_queue, dim=0)\n",
        "\n",
        "\n",
        "    # Đổi 'image' thành 'video'\n",
        "    def forward(self, video, text, alpha, idx):\n",
        "        B, F, C, H, W = video.shape\n",
        "\n",
        "        video_flat = video.view(B*F, C, H, W)\n",
        "\n",
        "        image_embeds_flat = self.visual_encoder(video_flat)\n",
        "        image_feat_flat = F.normalize(self.vision_proj(image_embeds_flat[:,0,:]),dim=-1)\n",
        "\n",
        "        # --- TỔNG HỢP BIỂU DIỄN VIDEO BẰNG AVERAGE POOLING ---\n",
        "        image_feat_per_frame = image_feat_flat.view(B, F, -1)\n",
        "        image_feat = image_feat_per_frame.mean(dim=1)\n",
        "\n",
        "        image_embeds_fusion = image_embeds_flat.view(B, F, image_embeds_flat.size(1), -1).mean(dim=1)\n",
        "\n",
        "        image_atts = torch.ones(image_embeds_fusion.size()[:-1],dtype=torch.long).to(video.device)\n",
        "\n",
        "        text_output = self.text_encoder(text.input_ids, attention_mask = text.attention_mask,\n",
        "                                        return_dict = True, mode = 'text')\n",
        "        text_embeds = text_output.last_hidden_state\n",
        "        text_feat = F.normalize(self.text_proj(text_embeds[:,0,:]),dim=-1)\n",
        "\n",
        "        idx = idx.view(-1,1)\n",
        "        idx_all = torch.cat([idx.t(), self.idx_queue.clone().detach()],dim=1)\n",
        "        pos_idx = torch.eq(idx, idx_all).float()\n",
        "        sim_targets = pos_idx / pos_idx.sum(1,keepdim=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self._momentum_update()\n",
        "            # Xử lý tương tự cho momentum model\n",
        "            image_embeds_m_flat = self.visual_encoder_m(video_flat)\n",
        "            image_feat_m_flat = F.normalize(self.vision_proj_m(image_embeds_m_flat[:,0,:]),dim=-1)\n",
        "            image_feat_m = image_feat_m_flat.view(B, F, -1).mean(dim=1) # Average pooling for momentum model\n",
        "            image_feat_all = torch.cat([image_feat_m.t(),self.image_queue.clone().detach()],dim=1)\n",
        "\n",
        "            text_output_m = self.text_encoder_m(text.input_ids, attention_mask = text.attention_mask,\n",
        "                                                return_dict = True, mode = 'text')\n",
        "            text_feat_m = F.normalize(self.text_proj_m(text_output_m.last_hidden_state[:,0,:]),dim=-1)\n",
        "            text_feat_all = torch.cat([text_feat_m.t(),self.text_queue.clone().detach()],dim=1)\n",
        "\n",
        "            if self.distill:\n",
        "                sim_i2t_m = image_feat_m @ text_feat_all / self.temp\n",
        "                sim_t2i_m = text_feat_m @ image_feat_all / self.temp\n",
        "\n",
        "                sim_i2t_targets = alpha * F.softmax(sim_i2t_m, dim=1) + (1 - alpha) * sim_targets\n",
        "                sim_t2i_targets = alpha * F.softmax(sim_t2i_m, dim=1) + (1 - alpha) * sim_targets\n",
        "\n",
        "        sim_i2t = image_feat @ text_feat_all / self.temp\n",
        "        sim_t2i = text_feat @ image_feat_all / self.temp\n",
        "\n",
        "        if self.distill:\n",
        "            loss_i2t = -torch.sum(F.log_softmax(sim_i2t, dim=1)*sim_i2t_targets,dim=1).mean()\n",
        "            loss_t2i = -torch.sum(F.log_softmax(sim_t2i, dim=1)*sim_t2i_targets,dim=1).mean()\n",
        "        else:\n",
        "            loss_i2t = -torch.sum(F.log_softmax(sim_i2t, dim=1)*sim_targets,dim=1).mean()\n",
        "            loss_t2i = -torch.sum(F.log_softmax(sim_t2i, dim=1)*sim_targets,dim=1).mean()\n",
        "\n",
        "        loss_ita = (loss_i2t+loss_t2i)/2\n",
        "\n",
        "        self._dequeue_and_enqueue(image_feat_m, text_feat_m, idx)\n",
        "\n",
        "        ###=================================###\n",
        "        # forward the positve image-text pair\n",
        "        # encoder_hidden_states và encoder_attention_mask giờ đại diện cho VIDEO\n",
        "        output_pos = self.text_encoder(encoder_embeds = text_embeds,\n",
        "                                        attention_mask = text.attention_mask,\n",
        "                                        encoder_hidden_states = image_embeds_fusion, # Dùng biểu diễn video đã tổng hợp\n",
        "                                        encoder_attention_mask = image_atts,\n",
        "                                        return_dict = True,\n",
        "                                        mode = 'fusion',\n",
        "                                       )\n",
        "        with torch.no_grad():\n",
        "            bs = video.size(0)\n",
        "            weights_i2t = F.softmax(sim_i2t[:,:bs]+1e-4,dim=1)\n",
        "            weights_t2i = F.softmax(sim_t2i[:,:bs]+1e-4,dim=1)\n",
        "\n",
        "            mask = torch.eq(idx, idx.T)\n",
        "            weights_i2t.masked_fill_(mask, 0)\n",
        "            weights_t2i.masked_fill_(mask, 0)\n",
        "\n",
        "        # select a negative video for each text\n",
        "        video_embeds_neg = []\n",
        "        for b in range(bs):\n",
        "            neg_idx = torch.multinomial(weights_t2i[b], 1).item()\n",
        "            video_embeds_neg.append(image_embeds_fusion[neg_idx]) # Dùng image_embeds_fusion\n",
        "        video_embeds_neg = torch.stack(video_embeds_neg,dim=0)\n",
        "\n",
        "        # select a negative text for each video\n",
        "        text_embeds_neg = []\n",
        "        text_atts_neg = []\n",
        "        for b in range(bs):\n",
        "            neg_idx = torch.multinomial(weights_i2t[b], 1).item()\n",
        "            text_embeds_neg.append(text_embeds[neg_idx])\n",
        "            text_atts_neg.append(text.attention_mask[neg_idx])\n",
        "        text_embeds_neg = torch.stack(text_embeds_neg,dim=0)\n",
        "        text_atts_neg = torch.stack(text_atts_neg,dim=0)\n",
        "\n",
        "        text_embeds_all = torch.cat([text_embeds, text_embeds_neg],dim=0)\n",
        "        text_atts_all = torch.cat([text.attention_mask, text_atts_neg],dim=0)\n",
        "\n",
        "        video_embeds_all = torch.cat([video_embeds_neg,image_embeds_fusion],dim=0) # Dùng image_embeds_fusion\n",
        "        video_atts_all = torch.cat([image_atts,image_atts],dim=0) # Dùng image_atts\n",
        "\n",
        "        output_neg = self.text_encoder(encoder_embeds = text_embeds_all,\n",
        "                                        attention_mask = text_atts_all,\n",
        "                                        encoder_hidden_states = video_embeds_all, # Dùng video_embeds_all\n",
        "                                        encoder_attention_mask = video_atts_all,\n",
        "                                        return_dict = True,\n",
        "                                        mode = 'fusion',\n",
        "                                       )\n",
        "\n",
        "        vl_embeddings = torch.cat([output_pos.last_hidden_state[:,0,:], output_neg.last_hidden_state[:,0,:]],dim=0)\n",
        "        vl_output = self.itm_head(vl_embeddings)\n",
        "\n",
        "        itm_labels = torch.cat([torch.ones(bs,dtype=torch.long),torch.zeros(2*bs,dtype=torch.long)],\n",
        "                               dim=0).to(video.device)\n",
        "        loss_itm = F.cross_entropy(vl_output, itm_labels)\n",
        "\n",
        "        return loss_ita, loss_itm\n",
        "\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def copy_params(self):\n",
        "        for model_pair in self.model_pairs:\n",
        "            for param, param_m in zip(model_pair[0].parameters(), model_pair[1].parameters()):\n",
        "                param_m.data.copy_(param.data)  # initialize\n",
        "                param_m.requires_grad = False  # not update by gradient\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _momentum_update(self):\n",
        "        for model_pair in self.model_pairs:\n",
        "            for param, param_m in zip(model_pair[0].parameters(), model_pair[1].parameters()):\n",
        "                param_m.data = param_m.data * self.momentum + param.data * (1. - self.momentum)\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _dequeue_and_enqueue(self, image_feat, text_feat, idx):\n",
        "        # gather keys before updating queue\n",
        "        image_feats = concat_all_gather(image_feat) # image_feat ở đây là video_feat đã tổng hợp\n",
        "        text_feats = concat_all_gather(text_feat)\n",
        "        idxs = concat_all_gather(idx)\n",
        "\n",
        "        batch_size = image_feats.shape[0]\n",
        "\n",
        "        ptr = int(self.queue_ptr)\n",
        "        assert self.queue_size % batch_size == 0  # for simplicity\n",
        "\n",
        "        # replace the keys at ptr (dequeue and enqueue)\n",
        "        self.image_queue[:, ptr:ptr + batch_size] = image_feats.T\n",
        "        self.text_queue[:, ptr:ptr + batch_size] = text_feats.T\n",
        "        self.idx_queue[:, ptr:ptr + batch_size] = idxs.T\n",
        "        ptr = (ptr + batch_size) % self.queue_size  # move pointer\n",
        "\n",
        "        self.queue_ptr[0] = ptr\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def concat_all_gather(tensor):\n",
        "    \"\"\"\n",
        "    Performs all_gather operation on the provided tensors.\n",
        "    *** Warning ***: torch.distributed.all_gather has no gradient.\n",
        "    \"\"\"\n",
        "    tensors_gather = [torch.ones_like(tensor)\n",
        "        for _ in range(torch.distributed.get_world_size())]\n",
        "    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\n",
        "\n",
        "    output = torch.cat(tensors_gather, dim=0)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B292uaTBP8-Z",
        "outputId": "aca65fd9-b327-47e2-f876-6ad8976f9b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/ALBEF/scheduler/scheduler_factory.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/ALBEF/scheduler/scheduler_factory.py\n",
        "# File: scheduler/scheduler_factory.py\n",
        "\n",
        "\"\"\" Scheduler Factory\n",
        "Hacked together by / Copyright 2020 Ross Wightman\n",
        "\"\"\"\n",
        "from .cosine_lr import CosineLRScheduler\n",
        "from .tanh_lr import TanhLRScheduler\n",
        "from .step_lr import StepLRScheduler\n",
        "from .plateau_lr import PlateauLRScheduler\n",
        "\n",
        "\n",
        "def create_scheduler(args, optimizer):\n",
        "    # Thay đổi tất cả các truy cập args['key'] thành args.get('key', default_value)\n",
        "    # Cung cấp giá trị mặc định phù hợp cho từng key\n",
        "    num_epochs = args.get('epochs', 10) # Default: 10 epochs\n",
        "\n",
        "    if args.get('lr_noise', None) is not None:\n",
        "        lr_noise = args.get('lr_noise')\n",
        "        if isinstance(lr_noise, (list, tuple)):\n",
        "            noise_range = [n * num_epochs for n in lr_noise]\n",
        "            if len(noise_range) == 1:\n",
        "                noise_range = noise_range[0]\n",
        "        else:\n",
        "            noise_range = lr_noise * num_epochs\n",
        "    else:\n",
        "        noise_range = None\n",
        "\n",
        "    lr_scheduler = None\n",
        "    if args.get('sched') == 'cosine': # Sử dụng .get()\n",
        "        lr_scheduler = CosineLRScheduler(\n",
        "            optimizer,\n",
        "            t_initial=num_epochs,\n",
        "            t_mul=args.get('lr_cycle_mul', 1.),\n",
        "            lr_min=args.get('min_lr', 0.), # Default: 0\n",
        "            decay_rate=args.get('decay_rate', 1.), # Default: 1\n",
        "            warmup_lr_init=args.get('warmup_lr', 0.), # Default: 0\n",
        "            warmup_t=args.get('warmup_epochs', 0), # THAY ĐỔI TRỌNG TÂM Ở ĐÂY, Default: 0\n",
        "            cycle_limit=args.get('lr_cycle_limit', 1),\n",
        "            t_in_epochs=True,\n",
        "            noise_range_t=noise_range,\n",
        "            noise_pct=args.get('lr_noise_pct', 0.67),\n",
        "            noise_std=args.get('lr_noise_std', 1.),\n",
        "            noise_seed=args.get('seed', 42),\n",
        "        )\n",
        "        num_epochs = lr_scheduler.get_cycle_length() + args.get('cooldown_epochs', 0) # Default: 0\n",
        "    elif args.get('sched') == 'tanh':\n",
        "        lr_scheduler = TanhLRScheduler(\n",
        "            optimizer,\n",
        "            t_initial=num_epochs,\n",
        "            t_mul=args.get('lr_cycle_mul', 1.),\n",
        "            lr_min=args.get('min_lr', 0.),\n",
        "            warmup_lr_init=args.get('warmup_lr', 0.),\n",
        "            warmup_t=args.get('warmup_epochs', 0),\n",
        "            cycle_limit=args.get('lr_cycle_limit', 1),\n",
        "            t_in_epochs=True,\n",
        "            noise_range_t=noise_range,\n",
        "            noise_pct=args.get('lr_noise_pct', 0.67),\n",
        "            noise_std=args.get('lr_noise_std', 1.),\n",
        "            noise_seed=args.get('seed', 42),\n",
        "        )\n",
        "        num_epochs = lr_scheduler.get_cycle_length() + args.get('cooldown_epochs', 0)\n",
        "    elif args.get('sched') == 'step':\n",
        "        lr_scheduler = StepLRScheduler(\n",
        "            optimizer,\n",
        "            decay_t=args.get('decay_epochs', num_epochs), # Default: num_epochs\n",
        "            decay_rate=args.get('decay_rate', 1.),\n",
        "            warmup_lr_init=args.get('warmup_lr', 0.),\n",
        "            warmup_t=args.get('warmup_epochs', 0),\n",
        "            noise_range_t=noise_range,\n",
        "            noise_pct=args.get('lr_noise_pct', 0.67),\n",
        "            noise_std=args.get('lr_noise_std', 1.),\n",
        "            noise_seed=args.get('seed', 42),\n",
        "        )\n",
        "    elif args.get('sched') == 'plateau':\n",
        "        mode = 'min' if 'loss' in args.get('eval_metric', '') else 'max'\n",
        "        lr_scheduler = PlateauLRScheduler(\n",
        "            optimizer,\n",
        "            decay_rate=args.get('decay_rate', 1.),\n",
        "            patience_t=args.get('patience_epochs', 10), # Default: 10\n",
        "            lr_min=args.get('min_lr', 0.),\n",
        "            mode=mode,\n",
        "            warmup_lr_init=args.get('warmup_lr', 0.),\n",
        "            warmup_t=args.get('warmup_epochs', 0),\n",
        "            cooldown_t=args.get('cooldown_epochs', 0), # Default: 0\n",
        "            noise_range_t=noise_range,\n",
        "            noise_pct=args.get('lr_noise_pct', 0.67),\n",
        "            noise_std=args.get('lr_noise_std', 1.),\n",
        "            noise_seed=args.get('seed', 42),\n",
        "        )\n",
        "\n",
        "    return lr_scheduler, num_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIanhtkNuBP3",
        "outputId": "5db05b55-9701-42b4-f174-74dc1448a6ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/ALBEF/Retrieval.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/ALBEF/Retrieval.py\n",
        "import argparse\n",
        "import os\n",
        "from ruamel.yaml import YAML # Đổi từ ruamel_yaml thành ruamel.yaml\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from models.model_retrieval import ALBEF\n",
        "from models.vit import interpolate_pos_embed\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "import utils\n",
        "from dataset import create_dataset, create_sampler, create_loader\n",
        "from scheduler import create_scheduler\n",
        "from optim import create_optimizer\n",
        "\n",
        "\n",
        "def train(model, data_loader, optimizer, tokenizer, epoch, warmup_steps, device, scheduler, config):\n",
        "    # train\n",
        "    model.train()\n",
        "\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    metric_logger.add_meter('loss_itm', utils.SmoothedValue(window_size=1, fmt='{value:.4f}'))\n",
        "    metric_logger.add_meter('loss_ita', utils.SmoothedValue(window_size=1, fmt='{value:.4f}'))\n",
        "    header = 'Train Epoch: [{}]'.format(epoch)\n",
        "    print_freq = 50\n",
        "    step_size = 100\n",
        "    warmup_iterations = warmup_steps*step_size\n",
        "\n",
        "    # data_loader giờ đây sẽ trả về video (5D tensor), text, và idx\n",
        "    for i,(video, text, idx) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
        "        video = video.to(device,non_blocking=True) # Thay image = image.to(device,...)\n",
        "        idx = idx.to(device,non_blocking=True)\n",
        "        text_input = tokenizer(text, padding='longest', max_length=30, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        if epoch>0 or not config['warm_up']:\n",
        "            alpha = config['alpha']\n",
        "        else:\n",
        "            alpha = config['alpha']*min(1,i/len(data_loader))\n",
        "\n",
        "        # Truyền video thay vì image vào model\n",
        "        loss_ita, loss_itm = model(video, text_input,alpha=alpha, idx=idx)\n",
        "        loss = loss_ita + loss_itm\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        metric_logger.update(loss_itm=loss_itm.item())\n",
        "        metric_logger.update(loss_ita=loss_ita.item())\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "        if epoch==0 and i%step_size==0 and i<=warmup_iterations:\n",
        "            scheduler.step(i//step_size)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger.global_avg())\n",
        "    return {k: \"{:.3f}\".format(meter.global_avg) for k, meter in metric_logger.meters.items()}\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluation(model, data_loader, tokenizer, device, config):\n",
        "    # test\n",
        "    model.eval()\n",
        "\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    header = 'Evaluation:'\n",
        "\n",
        "    print('Computing features for evaluation...')\n",
        "    start_time = time.time()\n",
        "\n",
        "    texts = data_loader.dataset.text\n",
        "    num_text = len(texts)\n",
        "    text_bs = 256\n",
        "    text_feats = []\n",
        "    text_embeds = []\n",
        "    text_atts = []\n",
        "    for i in range(0, num_text, text_bs):\n",
        "        text = texts[i: min(num_text, i+text_bs)]\n",
        "        text_input = tokenizer(text, padding='max_length', truncation=True, max_length=30, return_tensors=\"pt\").to(device)\n",
        "        text_output = model.text_encoder(text_input.input_ids, attention_mask = text_input.attention_mask, mode='text')\n",
        "        text_feat = text_output.last_hidden_state\n",
        "        text_embed = F.normalize(model.text_proj(text_feat[:,0,:]))\n",
        "        text_embeds.append(text_embed)\n",
        "        text_feats.append(text_feat)\n",
        "        text_atts.append(text_input.attention_mask)\n",
        "    text_embeds = torch.cat(text_embeds,dim=0)\n",
        "    text_feats = torch.cat(text_feats,dim=0)\n",
        "    text_atts = torch.cat(text_atts,dim=0)\n",
        "\n",
        "    video_feats = [] # Sẽ lưu trữ video_embeds_fusion\n",
        "    video_embeds = [] # Sẽ lưu trữ video_feat (từ average pooling)\n",
        "\n",
        "    # data_loader trả về video, video_id\n",
        "    for video, video_id in data_loader:\n",
        "        video = video.to(device) # Shape: (B, F, C, H, W)\n",
        "        B, F, C, H, W = video.shape\n",
        "        video_flat = video.view(B*F, C, H, W) # Flatten video frames\n",
        "\n",
        "        # Xử lý từng khung hình\n",
        "        image_feat_flat = model.visual_encoder(video_flat)\n",
        "\n",
        "        # Lấy CLS token của từng khung hình và chiếu (project) nó\n",
        "        image_embed_flat = model.vision_proj(image_feat_flat[:,0,:])\n",
        "        image_embed_flat = F.normalize(image_embed_flat,dim=-1)\n",
        "\n",
        "        # Tổng hợp biểu diễn video\n",
        "        # (B, N, D) - N là num_patches+1, D là embed_dim\n",
        "        video_feat_pooled = image_feat_flat.view(B, F, image_feat_flat.size(1), -1).mean(dim=1)\n",
        "        # (B, D) - D là embed_dim\n",
        "        video_embed_pooled = image_embed_flat.view(B, F, -1).mean(dim=1)\n",
        "\n",
        "        video_feats.append(video_feat_pooled)\n",
        "        video_embeds.append(video_embed_pooled)\n",
        "\n",
        "    # Concatenate tất cả các biểu diễn video đã tính toán\n",
        "    image_feats = torch.cat(video_feats,dim=0)\n",
        "    image_embeds = torch.cat(video_embeds,dim=0)\n",
        "\n",
        "    sims_matrix = image_embeds @ text_embeds.t()\n",
        "    # Kích thước ma trận score_matrix_i2t dựa trên số lượng video thực tế\n",
        "    score_matrix_i2t = torch.full((len(data_loader.dataset.video_ids),len(texts)),-100.0).to(device)\n",
        "\n",
        "    num_tasks = utils.get_world_size()\n",
        "    rank = utils.get_rank()\n",
        "    step = sims_matrix.size(0)//num_tasks + 1\n",
        "    start = rank*step\n",
        "    end = min(sims_matrix.size(0),start+step)\n",
        "\n",
        "    for i,sims in enumerate(metric_logger.log_every(sims_matrix[start:end], 50, header)):\n",
        "        topk_sim, topk_idx = sims.topk(k=config['k_test'], dim=0)\n",
        "\n",
        "        # encoder_output và encoder_att cần được tạo ra từ image_feats (biểu diễn video)\n",
        "        # image_feats[start+i] là biểu diễn video tổng hợp cho một video cụ thể\n",
        "        encoder_output = image_feats[start+i].unsqueeze(0).repeat(config['k_test'],1,1) # Add unsqueeze(0) để tạo batch dim\n",
        "        encoder_att = torch.ones(encoder_output.size()[:-1],dtype=torch.long).to(device)\n",
        "        output = model.text_encoder(encoder_embeds = text_feats[topk_idx],\n",
        "                                    attention_mask = text_atts[topk_idx],\n",
        "                                    encoder_hidden_states = encoder_output,\n",
        "                                    encoder_attention_mask = encoder_att,\n",
        "                                    return_dict = True,\n",
        "                                    mode = 'fusion'\n",
        "                                   )\n",
        "        score = model.itm_head(output.last_hidden_state[:,0,:])[:,1]\n",
        "        score_matrix_i2t[start+i,topk_idx] = score\n",
        "\n",
        "    sims_matrix = sims_matrix.t()\n",
        "    # Kích thước ma trận score_matrix_t2i dựa trên số lượng video thực tế\n",
        "    score_matrix_t2i = torch.full((len(texts),len(data_loader.dataset.video_ids)),-100.0).to(device)\n",
        "\n",
        "    step = sims_matrix.size(0)//num_tasks + 1\n",
        "    start = rank*step\n",
        "    end = min(sims_matrix.size(0),start+step)\n",
        "\n",
        "    for i,sims in enumerate(metric_logger.log_every(sims_matrix[start:end], 50, header)):\n",
        "\n",
        "        topk_sim, topk_idx = sims.topk(k=config['k_test'], dim=0)\n",
        "        encoder_output = image_feats[topk_idx]\n",
        "        encoder_att = torch.ones(encoder_output.size()[:-1],dtype=torch.long).to(device)\n",
        "        output = model.text_encoder(encoder_embeds = text_feats[start+i].repeat(config['k_test'],1,1),\n",
        "                                    attention_mask = text_atts[start+i].repeat(config['k_test'],1),\n",
        "                                    encoder_hidden_states = encoder_output,\n",
        "                                    encoder_attention_mask = encoder_att,\n",
        "                                    return_dict = True,\n",
        "                                    mode = 'fusion'\n",
        "                                   )\n",
        "        score = model.itm_head(output.last_hidden_state[:,0,:])[:,1]\n",
        "        score_matrix_t2i[start+i,topk_idx] = score\n",
        "\n",
        "    if args.distributed:\n",
        "        dist.barrier()\n",
        "        torch.distributed.all_reduce(score_matrix_i2t, op=torch.distributed.ReduceOp.SUM)\n",
        "        torch.distributed.all_reduce(score_matrix_t2i, op=torch.distributed.ReduceOp.SUM)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "    print('Evaluation time {}'.format(total_time_str))\n",
        "\n",
        "    return score_matrix_i2t.cpu().numpy(), score_matrix_t2i.cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def itm_eval(scores_i2t, scores_t2i, txt2img, img2txt):\n",
        "\n",
        "    #Images->Text\n",
        "    ranks = np.zeros(scores_i2t.shape[0])\n",
        "    for index,score in enumerate(scores_i2t):\n",
        "        inds = np.argsort(score)[::-1]\n",
        "        # Score\n",
        "        rank = 1e20\n",
        "        for i in img2txt[index]:\n",
        "            tmp = np.where(inds == i)[0][0]\n",
        "            if tmp < rank:\n",
        "                rank = tmp\n",
        "        ranks[index] = rank\n",
        "\n",
        "    # Compute metrics\n",
        "    tr1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
        "    tr5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
        "    tr10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n",
        "\n",
        "    #Text->Images\n",
        "    ranks = np.zeros(scores_t2i.shape[0])\n",
        "\n",
        "    for index,score in enumerate(scores_t2i):\n",
        "        inds = np.argsort(score)[::-1]\n",
        "        ranks[index] = np.where(inds == txt2img[index])[0][0]\n",
        "\n",
        "    # Compute metrics\n",
        "    ir1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
        "    ir5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
        "    ir10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n",
        "\n",
        "    tr_mean = (tr1 + tr5 + tr10) / 3\n",
        "    ir_mean = (ir1 + ir5 + ir10) / 3\n",
        "    r_mean = (tr_mean + ir_mean) / 2\n",
        "\n",
        "    eval_result =  {'txt_r1': tr1,\n",
        "                    'txt_r5': tr5,\n",
        "                    'txt_r10': tr10,\n",
        "                    'txt_r_mean': tr_mean,\n",
        "                    'img_r1': ir1,\n",
        "                    'img_r5': ir5,\n",
        "                    'img_r10': ir10,\n",
        "                    'img_r_mean': ir_mean,\n",
        "                    'r_mean': r_mean}\n",
        "    return eval_result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main(args, config):\n",
        "    utils.init_distributed_mode(args)\n",
        "\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    # fix the seed for reproducibility\n",
        "    seed = args.seed + utils.get_rank()\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    #### Dataset ####\n",
        "    print(\"Creating retrieval dataset\")\n",
        "    train_dataset, val_dataset, test_dataset = create_dataset('re', config)\n",
        "\n",
        "    if args.distributed:\n",
        "        num_tasks = utils.get_world_size()\n",
        "        global_rank = utils.get_rank()\n",
        "        samplers = create_sampler([train_dataset], [True], num_tasks, global_rank) + [None, None]\n",
        "    else:\n",
        "        samplers = [None, None, None]\n",
        "\n",
        "    train_loader, val_loader, test_loader = create_loader([train_dataset, val_dataset, test_dataset],samplers,\n",
        "                                                          batch_size=[config['batch_size_train']]+[config['batch_size_test']]*2,\n",
        "                                                          num_workers=[4,4,4],\n",
        "                                                          is_trains=[True, False, False],\n",
        "                                                          collate_fns=[None,None,None])\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(args.text_encoder)\n",
        "\n",
        "    #### Model ####\n",
        "    print(\"Creating model\")\n",
        "    if 'num_frames_per_video' not in config:\n",
        "        raise ValueError(\"num_frames_per_video must be specified in the config file.\")\n",
        "    model = ALBEF(config=config, text_encoder=args.text_encoder, tokenizer=tokenizer)\n",
        "\n",
        "    if args.checkpoint:\n",
        "        checkpoint = torch.load(args.checkpoint, map_location='cpu')\n",
        "        state_dict = checkpoint['model']\n",
        "\n",
        "        # reshape positional embedding to accomodate for image resolution change\n",
        "        pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder)\n",
        "        state_dict['visual_encoder.pos_embed'] = pos_embed_reshaped\n",
        "        m_pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],model.visual_encoder_m)\n",
        "        state_dict['visual_encoder_m.pos_embed'] = m_pos_embed_reshaped\n",
        "\n",
        "        for key in list(state_dict.keys()):\n",
        "            if 'bert' in key:\n",
        "                encoder_key = key.replace('bert.','')\n",
        "                state_dict[encoder_key] = state_dict[key]\n",
        "                del state_dict[key]\n",
        "        msg = model.load_state_dict(state_dict,strict=False)\n",
        "\n",
        "        print('load checkpoint from %s'%args.checkpoint)\n",
        "        print(msg)\n",
        "\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    model_without_ddp = model\n",
        "    if args.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
        "        model_without_ddp = model.module\n",
        "\n",
        "    arg_opt = utils.AttrDict(config['optimizer'])\n",
        "    optimizer = create_optimizer(arg_opt, model)\n",
        "    arg_sche = utils.AttrDict(config['schedular']) # Đây là AttrDict cho scheduler\n",
        "\n",
        "    # Sửa đổi tại đây để truy cập an toàn hơn\n",
        "    # Dòng này đã được sửa trong scheduler_factory.py, nhưng cần sửa cả ở đây\n",
        "    # vì biến `warmup_steps` vẫn được dùng ở `lr_scheduler.step(epoch+warmup_steps+1)`\n",
        "    max_epoch = arg_sche.get('epochs', 10) # Sử dụng .get() trên AttrDict\n",
        "    warmup_steps = arg_sche.get('warmup_epochs', 1) # Sử dụng .get() trên AttrDict\n",
        "\n",
        "    # Gọi create_scheduler với arg_sche\n",
        "    lr_scheduler, _ = create_scheduler(arg_sche, optimizer)\n",
        "\n",
        "    best = 0\n",
        "    best_epoch = 0\n",
        "\n",
        "    print(\"Start training\")\n",
        "    start_time = time.time()\n",
        "    for epoch in range(0, max_epoch):\n",
        "        if not args.evaluate:\n",
        "            if args.distributed:\n",
        "                train_loader.sampler.set_epoch(epoch)\n",
        "            train_stats = train(model, train_loader, optimizer, tokenizer, epoch, warmup_steps, device, lr_scheduler, config)\n",
        "\n",
        "        score_val_i2t, score_val_t2i, = evaluation(model_without_ddp, val_loader, tokenizer, device, config)\n",
        "        score_test_i2t, score_test_t2i = evaluation(model_without_ddp, test_loader, tokenizer, device, config)\n",
        "\n",
        "        if utils.is_main_process():\n",
        "\n",
        "            val_result = itm_eval(score_val_i2t, score_val_t2i, val_loader.dataset.txt2img, val_loader.dataset.img2txt)\n",
        "            print(val_result)\n",
        "            test_result = itm_eval(score_test_i2t, score_test_t2i, test_loader.dataset.txt2img, test_loader.dataset.img2txt)\n",
        "            print(test_result)\n",
        "\n",
        "            if args.evaluate:\n",
        "                log_stats = {**{f'val_{k}': v for k, v in val_result.items()},\n",
        "                             **{f'test_{k}': v for k, v in test_result.items()},\n",
        "                             'epoch': epoch,\n",
        "                            }\n",
        "                with open(os.path.join(args.output_dir, \"log.txt\"),\"a\") as f:\n",
        "                    f.write(json.dumps(log_stats) + \"\\n\")\n",
        "            else:\n",
        "                log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
        "                             **{f'val_{k}': v for k, v in val_result.items()},\n",
        "                             **{f'test_{k}': v for k, v in test_result.items()},\n",
        "                             'epoch': epoch,\n",
        "                            }\n",
        "                with open(os.path.join(args.output_dir, \"log.txt\"),\"a\") as f:\n",
        "                    f.write(json.dumps(log_stats) + \"\\n\")\n",
        "\n",
        "                if val_result['r_mean']>best:\n",
        "                    save_obj = {\n",
        "                        'model': model_without_ddp.state_dict(),\n",
        "                        'optimizer': optimizer.state_dict(),\n",
        "                        'lr_scheduler': lr_scheduler.state_dict(),\n",
        "                        'config': config,\n",
        "                        'epoch': epoch,\n",
        "                    }\n",
        "                    torch.save(save_obj, os.path.join(args.output_dir, 'checkpoint_best.pth'))\n",
        "                    best = val_result['r_mean']\n",
        "                    best_epoch = epoch\n",
        "\n",
        "        if args.evaluate:\n",
        "            break\n",
        "\n",
        "        # Sử dụng `warmup_steps` đã được lấy một cách an toàn\n",
        "        lr_scheduler.step(epoch + warmup_steps + 1)\n",
        "        dist.barrier()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "    print('Training time {}'.format(total_time_str))\n",
        "\n",
        "    if utils.is_main_process():\n",
        "        with open(os.path.join(args.output_dir, \"log.txt\"),\"a\") as f:\n",
        "            f.write(\"best epoch: %d\"%best_epoch)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', default='./configs/Retrieval_msrvtt.yaml')\n",
        "    parser.add_argument('--output_dir', default='output/Retrieval_msrvtt_baseline')\n",
        "    parser.add_argument('--checkpoint', default='')\n",
        "    parser.add_argument('--text_encoder', default='bert-base-uncased')\n",
        "    parser.add_argument('--evaluate', action='store_true')\n",
        "    parser.add_argument('--device', default='cuda')\n",
        "    parser.add_argument('--seed', default=42, type=int)\n",
        "    parser.add_argument('--world_size', default=1, type=int, help='number of distributed processes')\n",
        "    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
        "    parser.add_argument('--distributed', default=True, type=bool)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(f\"Loading config from: {args.config}\")\n",
        "    yaml_parser = YAML(typ='safe')\n",
        "    try:\n",
        "        with open(args.config, 'r') as f:\n",
        "            config = yaml_parser.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Config file not found at {args.config}\")\n",
        "        exit(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading config file: {e}\")\n",
        "        exit(1)\n",
        "\n",
        "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    yaml_parser.dump(config, open(os.path.join(args.output_dir, 'config.yaml'), 'w'))\n",
        "\n",
        "    main(args, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2fKjmuJ0YVf",
        "outputId": "0fea569c-3bdd-40f1-e072-63b8da854cd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ALBEF\n",
            "/usr/local/lib/python3.11/dist-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  main()\n",
            "2025-06-11 06:47:34.346730: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1749624454.572187    8937 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1749624454.647975    8937 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-11 06:47:35.185415: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading config from: ./configs/Retrieval_msrvtt.yaml\n",
            "| distributed init (rank 0): env://\n",
            "[rank0]:[W611 06:47:41.256845029 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
            "Creating retrieval dataset\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Creating model\n",
            "reshape position embedding from 256 to 196\n",
            "reshape position embedding from 256 to 196\n",
            "load checkpoint from /content/ALBEF/ALBEF.pth\n",
            "_IncompatibleKeys(missing_keys=['idx_queue'], unexpected_keys=['text_encoder.cls.predictions.bias', 'text_encoder.cls.predictions.transform.dense.weight', 'text_encoder.cls.predictions.transform.dense.bias', 'text_encoder.cls.predictions.transform.LayerNorm.weight', 'text_encoder.cls.predictions.transform.LayerNorm.bias', 'text_encoder.cls.predictions.decoder.weight', 'text_encoder.cls.predictions.decoder.bias', 'text_encoder_m.cls.predictions.bias', 'text_encoder_m.cls.predictions.transform.dense.weight', 'text_encoder_m.cls.predictions.transform.dense.bias', 'text_encoder_m.cls.predictions.transform.LayerNorm.weight', 'text_encoder_m.cls.predictions.transform.LayerNorm.bias', 'text_encoder_m.cls.predictions.decoder.weight', 'text_encoder_m.cls.predictions.decoder.bias'])\n",
            "Start training\n",
            "/content/ALBEF/dataset/randaugment.py:31: RuntimeWarning: overflow encountered in scalar negative\n",
            "  offset = -low * scale\n",
            "/content/ALBEF/dataset/randaugment.py:31: RuntimeWarning: overflow encountered in scalar negative\n",
            "  offset = -low * scale\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/content/ALBEF/Retrieval.py\", line 418, in <module>\n",
            "[rank0]:     main(args, config)\n",
            "[rank0]:   File \"/content/ALBEF/Retrieval.py\", line 332, in main\n",
            "[rank0]:     train_stats = train(model, train_loader, optimizer, tokenizer, epoch, warmup_steps, device, lr_scheduler, config)  \n",
            "[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/content/ALBEF/Retrieval.py\", line 53, in train\n",
            "[rank0]:     loss_ita, loss_itm = model(video, text_input,alpha=alpha, idx=idx)\n",
            "[rank0]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/distributed.py\", line 1643, in forward\n",
            "[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)\n",
            "[rank0]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/distributed.py\", line 1459, in _run_ddp_forward\n",
            "[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/content/ALBEF/models/model_retrieval.py\", line 72, in forward\n",
            "[rank0]:     image_embeds_flat = self.visual_encoder(video_flat)\n",
            "[rank0]:                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/content/ALBEF/models/vit.py\", line 171, in forward\n",
            "[rank0]:     x = blk(x, register_blk==i)\n",
            "[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/content/ALBEF/models/vit.py\", line 93, in forward\n",
            "[rank0]:     x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
            "[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/content/ALBEF/models/vit.py\", line 24, in forward\n",
            "[rank0]:     x = self.fc1(x)\n",
            "[rank0]:         ^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "[rank0]:     return F.linear(input, self.weight, self.bias)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 888.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 222.12 MiB is free. Process 98341 has 14.52 GiB memory in use. Of the allocated memory 13.35 GiB is allocated by PyTorch, and 742.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "[rank0]:[W611 06:48:39.820908275 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "E0611 06:48:41.989000 8913 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 8937) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launch.py\", line 208, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typing_extensions.py\", line 2950, in wrapper\n",
            "    return arg(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launch.py\", line 204, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launch.py\", line 189, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 909, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "Retrieval.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-06-11_06:48:41\n",
            "  host      : 585495dfd37c\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 8937)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ALBEF\n",
        "\n",
        "!python -m torch.distributed.launch --nproc_per_node=1 --use_env Retrieval.py \\\n",
        "--config ./configs/Retrieval_msrvtt.yaml \\\n",
        "--output_dir output/Retrieval_msrvtt_baseline \\\n",
        "--checkpoint /content/ALBEF/ALBEF.pth\n",
        "\n",
        "\"\"\"\n",
        "torchrun --nproc_per_node=2 Retrieval.py --config ./configs/Retrieval_msrvtt.yaml --output_dir output/Retrieval_msrvtt_ddp --checkpoint mscoco.pth \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jxa18ApSx0Ag"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
